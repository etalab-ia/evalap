import copy
import os
from datetime import datetime

import requests
import streamlit as st
from template_manager import TemplateManager
from ui_components import (
    copy_to_clipboard_button,
    init_page_styles,
    validation_status_indicator,
)
from utils import fetch

EVALAP_FRONTEND_TOKEN = os.getenv("EVALAP_FRONTEND_TOKEN")

# --- Configuration ---

PROMPT_TEMPLATES = {
    "agent_public_formel": {
        "label": "Agent public - Ton formel",
        "content": """RÃ´le : Tu es un assistant conversationnel basÃ© sur l'IA, destinÃ© Ã  accompagner les agents publics dans leurs missions quotidiennes. Tu fournis des rÃ©ponses fiables, pÃ©dagogiques et adaptÃ©es au contexte de la fonction publique.

Principes fondamentaux :
* VÃ©racitÃ© et prÃ©cision : Ne fournis que des informations exactes, Ã©tayÃ©es et cohÃ©rentes avec les rÃ¨gles applicables dans la fonction publique et l'administration publique.
* Principe de prudence :
   * Si tu ne sais pas, tu ne rÃ©ponds pas.
   * Signale ton incertitude et propose des pistes de vÃ©rification ou des sources officielles lorsque c'est pertinent.
* NeutralitÃ© et impartialitÃ© : Reste strictement neutre, non partisan et conforme aux valeurs du service public.
* ClartÃ© et pÃ©dagogie : Explique les concepts de maniÃ¨re simple, structurÃ©e et comprÃ©hensible par tout type d'agent, quel que soit son niveau d'expertise.
* Respect de la confidentialitÃ© : Ne traite pas de donnÃ©es personnelles sensibles et Ã©vite toute rÃ©ponse pouvant conduire Ã  une fuite ou une interprÃ©tation dangereuse d'informations confidentielles.
* Limitation de ton rÃ´le :
   * Tu n'effectues pas d'actes juridiques, administratifs ou rÃ©glementaires.
   * Tu ne remplaces pas l'avis d'un juriste, d'un supÃ©rieur hiÃ©rarchique ou d'une autoritÃ© compÃ©tente.
   * Tu ne proposes pas de dÃ©cisions engageantes.

Comportement attendu :
* Reformule la demande si elle est ambiguÃ«.
* Donne des rÃ©ponses concises mais complÃ¨tes.
* Cite les rÃ¨gles ou textes applicables lorsque tu les connais.
* Si plusieurs interprÃ©tations existent, prÃ©sente les options et leurs implications.
* En cas d'incertitude, utilise une reformulation du type : "Je ne dispose pas d'informations suffisamment fiables pour rÃ©pondre avec certitude Ã  cette question."
""",
    },
    "agent_public_familier": {
        "label": "Agent public - Ton familier",
        "content": """Tu es un assistant conÃ§u pour filer un coup de main aux agents publics dans leurs missions. Ton rÃ´le : rÃ©flÃ©chir avec eux, proposer des pistes, clarifier les idÃ©es, et aider Ã  construire des solutions â€” sans jamais inventer n'importe quoi.

RÃ¨gles essentielles :
* Si tu ne sais pas, tu ne rÃ©ponds pas. Tu le dis clairement et tu proposes oÃ¹ chercher plutÃ´t que de broder.
* Tu restes cool, clair et direct : pas de jargon inutile, pas de ton trop formel.
* Tu aides Ã  explorer des idÃ©es : tu proposes des angles de rÃ©flexion, tu questionnes, tu reformules, tu aides Ã  structurer.
* Tu restes neutre, fiable et alignÃ© avec les valeurs du service public.
* Tu n'es pas lÃ  pour faire de la rÃ©glementation "au mot prÃ¨s" ni pour prendre des dÃ©cisions officielles : tu donnes des pistes, pas des actes administratifs.
* Tes rÃ©ponses doivent Ãªtre simples, digestes, pratiques â€” comme si tu Ã©changeais avec un collÃ¨gue autour d'un tableau blanc.
""",
    },
}

COLLECTION_TO_DATASET = {
    "service-public": "test_service_public",
    "annuaire-administrations-etat": "test_annuaire_entreprises",
}

DEFAULT_MODEL = "openweight-small"
DEFAULT_JUDGE_MODEL = "openweight-medium"
CHOICE_METRICS = ["judge_notator", "judge_exactness"]
DEFAULT_METRICS = [
    "generation_time",
    "nb_tokens_prompt",
    "nb_tokens_completion",
    "energy_consumption",
    "gwp_consumption",
]
DEFAULT_TEMPERATURE = 0.2
ALBERT_PROVIDER_URL = "https://albert.api.etalab.gouv.fr/v1"
DEFAULT_METHOD_COLLECTION = "semantic"

template_manager = TemplateManager()


# --- Utilitary Functions ---


def get_public_collections(api_key):
    url = f"{ALBERT_PROVIDER_URL}/collections"
    headers = {
        "accept": "application/json",
        "Authorization": f"Bearer {api_key}",
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        data = response.json()
        public_cols = [
            {"id": collection["id"], "name": collection["name"]}
            for collection in data.get("data", [])
            if collection.get("visibility") == "public"
            and collection.get("name") in COLLECTION_TO_DATASET.keys()
        ]
        return public_cols
    else:
        st.error(f"Error fetching collections: {response.status_code}")
        return []


def _should_skip_dataset(dataset: dict) -> bool:
    return "output" in dataset.get("columns", [])


def list_datasets() -> list[str]:
    datasets = fetch("get", "/datasets")
    if not datasets:
        st.warning("No dataset available")
        return []

    # Use mapping values as a whitelist
    allowed = set(COLLECTION_TO_DATASET.values())

    filtered = [ds["name"] for ds in datasets if ds["name"] in allowed and not _should_skip_dataset(ds)]

    if not filtered:
        st.warning("No datasets available among the filtered datasets.")
    return filtered


def get_default_dataset(collections_selected_names, available_datasets):
    for collection_name in collections_selected_names:
        if collection_name in COLLECTION_TO_DATASET:
            candidate = COLLECTION_TO_DATASET[collection_name]
            if candidate in available_datasets:
                return candidate
    return None


def post_experiment_set(expset, EVALAP_FRONTEND_TOKEN):
    return fetch("post", "/experiment_set", data=expset, token=EVALAP_FRONTEND_TOKEN)


def create_experiment_set(
    dataset,
    collection_ids,
    model_name,
    prompt,
    judge_model,
    api_key,
    metrics,
):
    model_config = {
        "name": model_name,
        "base_url": ALBERT_PROVIDER_URL,
        "api_key": api_key,
        "system_prompt": prompt.strip(),
        "sampling_params": {"temperature": DEFAULT_TEMPERATURE},
    }
    if collection_ids:
        model_config["extra_params"] = {
            "search": True,
            "search_args": {
                "method": DEFAULT_METHOD_COLLECTION,
                "collections": collection_ids,
                "k": 10,
            },
        }

    expset_name = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_launch_test_evaluation"

    expset = {
        "name": expset_name,
        "readme": "Experiment for_launch test",
        "cv": {
            "common_params": {
                "dataset": dataset,
                "metrics": metrics,
                "judge_model": {
                    "name": judge_model,
                    "base_url": ALBERT_PROVIDER_URL,
                    "api_key": api_key,
                },
            },
            "grid_params": {"model": [model_config]},
            "repeat": 1,
        },
    }

    return expset


# --- UI ---


def validate_api_key(api_key):
    if not api_key:
        return False, None

    try:
        url = f"{ALBERT_PROVIDER_URL}/collections"
        headers = {
            "accept": "application/json",
            "Authorization": f"Bearer {api_key}",
        }
        response = requests.get(url, headers=headers, timeout=5)

        if response.status_code == 200:
            return True, None
        else:
            return False, "Invalid API key"
    except requests.exceptions.Timeout:
        return False, "Request timeout"
    except Exception as e:
        return False, f"Connection error: {str(e)}"


def render_api_key_section():
    st.subheader("Access to Albert API to run a test")

    col_key1, col_key2 = st.columns([5, 5])

    with col_key1:
        st.markdown(
            "<p style='margin-bottom: 0px;'> Already have an API key? Enter it here </p>",
            unsafe_allow_html=True,
        )

    with col_key2:
        col_input, col_status = st.columns([4, 2])

        with col_input:
            user_api_key = st.text_input(
                " ",
                type="password",
                key="user_api_key_launch",
                label_visibility="collapsed",
            )

        is_valid = False
        error_msg = None

        if user_api_key:
            is_valid, error_msg = validate_api_key(user_api_key)

        with col_status:
            if user_api_key:
                validation_status_indicator(is_valid, error_msg)

    if not is_valid:
        st.write(
            "Donâ€™t have one yet? Request it via the  "
            "[contact form](https://albert.sites.beta.gouv.fr/access/) "
            "- weâ€™ll get back to you within 24 working hours."
        )

    return user_api_key, is_valid


def render_ai_system_config(user_api_key, is_api_key_valid):
    st.subheader("Set up the AI system you want to test")

    col1, col2, col3 = st.columns(3)

    with col1:
        if is_api_key_valid:
            public_collections = get_public_collections(user_api_key)
            if not public_collections:
                st.info("No matching public collections available for this API key.")
                collections_selected_names = []
                collections_selected_ids = []
            else:
                collection_names = [col["name"] for col in public_collections]
                collections_selected_names = st.multiselect(
                    "Public Collections",
                    options=collection_names,
                    key="main_collection_select",
                    help=("Select the knowledge base the test RAG system will use to retrieve information."),
                )
                collections_selected_ids = [
                    col["id"] for col in public_collections if col["name"] in collections_selected_names
                ]
        else:
            st.multiselect(
                "Collections",
                options=[],
                key="main_collection_select",
                disabled=True,
                help=("Select the knowledge base the test RAG system will use to retrieve information."),
            )
            collections_selected_names = []
            collections_selected_ids = []

    with col2:
        st.selectbox(
            "Model",
            DEFAULT_MODEL,
            key="selected_llm",
            disabled=not is_api_key_valid,
            help="Choose the LLM that will generate answers based on the collection.",
        )

    with col3:
        selected_prompt_content = render_prompt_selector(is_api_key_valid)

    return collections_selected_ids, collections_selected_names, selected_prompt_content


def render_prompt_selector(is_api_key_valid):
    prompt_labels = ["Select prompt"] + [v["label"] for v in PROMPT_TEMPLATES.values()]
    selected_prompt_label = st.selectbox(
        "Select a prompt",
        prompt_labels,
        key="selected_prompt",
        disabled=not is_api_key_valid,
        help="Pick the prompt template that guides how the model should respond.",
    )

    if selected_prompt_label and selected_prompt_label != "Select prompt":
        for prompt_data in PROMPT_TEMPLATES.values():
            if prompt_data["label"] == selected_prompt_label:
                return prompt_data["content"]

    return None


def render_evaluation_params(collections_selected_names, is_api_key_valid):
    st.subheader("Choose your evaluation parameter")

    col4, col5, col6 = st.columns(3)

    with col4:
        datasets = list_datasets() if is_api_key_valid else []
        default_dataset = get_default_dataset(collections_selected_names, datasets) if datasets else None
        dataset = st.selectbox(
            "Dataset",
            ["Select dataset"] + datasets,
            index=(datasets.index(default_dataset) + 1) if default_dataset else 0,
            key="main_dataset_select",
            disabled=not is_api_key_valid,
            help="Choose the reference dataset that will be used to evaluate the test RAG system.",
        )

    with col5:
        st.selectbox(
            "Judge Model",
            DEFAULT_JUDGE_MODEL,
            key="selected_judge",
            disabled=not is_api_key_valid,
            help="Select the LLM that will assess and score the systemâ€™s answers.",
        )

    with col6:
        selected_metrics = st.multiselect(
            "Metrics",
            options=CHOICE_METRICS,
            key="selected_metrics",
            disabled=not is_api_key_valid,
            help="Select the evaluation metrics you want to run on test system.",
            placeholder="Select metrics",
        )

    metrics = (selected_metrics if selected_metrics else []) + DEFAULT_METRICS

    return dataset, metrics


def mask_api_keys_in_experimentset(experimentset):
    exp_for_code = copy.deepcopy(experimentset)

    if "cv" in exp_for_code and "grid_params" in exp_for_code["cv"]:
        for model_cfg in exp_for_code["cv"]["grid_params"].get("model", []):
            if "api_key" in model_cfg:
                model_cfg["api_key"] = "YOUR_MODEL_API_KEY"

    if "cv" in exp_for_code and "common_params" in exp_for_code["cv"]:
        judge_model = exp_for_code["cv"]["common_params"].get("judge_model")
        if isinstance(judge_model, dict) and "api_key" in judge_model:
            judge_model["api_key"] = "YOUR_MODEL_API_KEY"

    return exp_for_code


def render_copy_code_popover(experimentset):
    with st.popover("ðŸ“‹ Copy code"):
        try:
            exp_for_code = mask_api_keys_in_experimentset(experimentset)

            st.markdown(
                "This code allows you to reproduce an experiment set.  \n"
                "**Caution**: the code might be incomplete, review it carefully "
                "and uses it at your own risks"
            )

            col1, col2 = st.columns([0.7, 0.3])

            with col1:
                copy_format = st.radio("Format:", ["Python", "cURL"], key="copy_format")

            if copy_format == "Python":
                code = template_manager.render_python(**exp_for_code)
                lang = "python"
            else:
                code = template_manager.render_curl(**exp_for_code)
                lang = "bash"

            with col2:
                copy_to_clipboard_button(code, button_id="copy_btn")

            st.code(code, language=lang)

        except Exception as e:
            st.error(f"Failed to render copy code template: {e}")


def handle_run_evaluation(experimentset, is_valid):
    if not is_valid:
        st.error("Please enter your access key before starting an assessment.")
        return

    try:
        result = post_experiment_set(experimentset, EVALAP_FRONTEND_TOKEN)

        if result and "id" in result:
            expset_id = result["id"]
            st.success(f"Experiment set created: {result['name']} (ID: {expset_id})")

            dashboard_url = f"/experiments_set?expset={expset_id}"
            st.markdown(
                f'<a href="{dashboard_url}" class="custom-button">See detailed results in the dashboard</a>',
                unsafe_allow_html=True,
            )
        else:
            st.error("Error creating experiment set")

    except Exception as e:
        st.error(f"Error creating experiment set: {e}")


# --- Main ---


def render_launch_tab():
    init_page_styles()

    user_api_key, is_api_key_valid = render_api_key_section()

    collections_ids, collections_names, prompt_content = render_ai_system_config(
        user_api_key, is_api_key_valid
    )

    dataset, metrics = render_evaluation_params(collections_names, is_api_key_valid)

    st.divider()

    experimentset = None
    try:
        experimentset = create_experiment_set(
            dataset=dataset,
            collection_ids=collections_ids,
            model_name=DEFAULT_MODEL,
            prompt=prompt_content or "",
            judge_model=DEFAULT_JUDGE_MODEL,
            api_key=user_api_key,
            metrics=metrics,
        )
    except Exception as e:
        st.error(f"Error creating experiment set: {e}")

    empty_col, button_col1, button_col2 = st.columns([8, 3, 3])

    with button_col1:
        run_button = st.button(
            "Run test evaluation",
            key="eval_button",
            disabled=not is_api_key_valid,
            help="Enter a valid API key to run evaluation" if not is_api_key_valid else None,
        )

    with button_col2:
        if experimentset:
            render_copy_code_popover(experimentset)

    if run_button and is_api_key_valid:
        handle_run_evaluation(experimentset, is_api_key_valid)
