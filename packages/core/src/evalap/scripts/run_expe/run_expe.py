#!/usr/bin/env python

"""
Run an experiment on evalap

Usage:
    run_expe.py --run-name=<name> [--evalap-token=<token>] [--expset=<id>] [--expe-name=<name>]

Options:
    --expe-name=<name>            Name of the experiment set
    --run-name=<name>             name of the model generation to load (result generated by evalap_run_answers).
    --expset=<id>                 Existing experiment set id to patch.
    --evalap-token=<token>           Authorization token for EVALAP-API access.
    -h --help                     Show this help message and exit.

Examples:
    evalap_run_expe --expe-name mfs_vllm_arena --run-name gemma-3-27b_mfs
"""

import glob
import json
import os
import re

import pandas as pd
import requests
from docopt import docopt

from evalap.core.utils import log_and_raise_for_status

evalap_url = "https://evalap.etalab.gouv.fr/v1"


def run_expe(args):
    run_name = args["--run-name"]
    expid = args["--expset"]
    expe_name = args["--expe-name"] or run_name
    evalap_token = args["--evalap-token"] or os.getenv("EVALAP_API_KEY")
    # Build the expset.
    # POST the expset (or expe if run is alone, @TODO).

    results = []
    model_raw = None
    # iterate all json file under results that starts with {run_name}
    for file_path in glob.glob(f"results/{run_name}*.json"):
        with open(file_path, "r") as f:
            data = json.load(f)

        if re.search(r"__\d+\.json$", file_path):
            # accumulate model result in a dataframe if ends by __{number}.json
            df = pd.DataFrame(data).sort_values(by="num_line")
            results.append(df)
        elif file_path.endswith("__details.json"):
            # set the model_raw object if files ends with __details.json
            model_raw = data

    # Build the expset
    # --
    metrics = [
        "answer_relevancy",
        "judge_exactness",
        "judge_notator",
        "output_length",
        "generation_time",
    ]

    common_params = {
        "dataset": model_raw.pop("dataset", "unknown_dataset"),  # Watch-out the pop !
        "model": model_raw,
        "metrics": metrics,
        "judge_model": "gpt-4o",
    }

    grid_params = {
        "model": [
            {
                "output": result["output"].tolist(),
                "execution_time": result["execution_time"].astype(int).tolist(),
            }
            for result in results
        ],
    }

    expset = {
        "name": expe_name,
        "readme": f"Experiment set for {expe_name}",
        "cv": {"common_params": common_params, "grid_params": grid_params, "repeat": 1},
    }

    headers = {
        "Authorization": f"Bearer {evalap_token}",
        "Content-Encoding": "gzip",
    }
    if expid is None:
        # POST the expset
        response = requests.post(f"{evalap_url}/experiment_set", json=expset, headers=headers)
        log_and_raise_for_status(response, "EVALAP error")
        resp = response.json()
    else:
        # PATCH the expset
        response = requests.patch(f"{evalap_url}/experiment_set/{expid}", json=expset, headers=headers)
        log_and_raise_for_status(response, "EVALAP error")
        resp = response.json()

    if "id" in resp:
        # expset_id = resp["id"]
        print(f"Created expset: {resp['name']} ({resp['id']})")
    else:
        print(resp)


def main():
    args = docopt(__doc__)
    run_expe(args)


if __name__ == "__main__":
    main()
