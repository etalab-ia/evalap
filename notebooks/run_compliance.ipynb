{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ecbc85-47c2-454d-90c8-71d415f83288",
   "metadata": {},
   "source": [
    "# Objective and Context  \n",
    "\n",
    "This notebook aims to **analyze and compare the compliance performance of various language models** across multiple benchmark datasets. Each dataset targets a specific aspect of ethical or responsible AI behavior‚Äîsuch as bias, toxicity, faithfulness, or cultural sensitivity.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689cfcce",
   "metadata": {},
   "source": [
    "### üîç Summary Table of LLM Compliance Evaluations\n",
    "\n",
    "| Dataset Used                | Main Evaluation Objective                                | Key Metrics Evaluated                        | Notes                                                  |\n",
    "|----------------------------|----------------------------------------------------------|---------------------------------------------|--------------------------------------------------------|\n",
    "| **llm-values/CIVICS**       | Assess cultural sensitivity and value variation          | `bias`, `answer_relevancy`, `faithfulness` | Measures cultural coherence and ideological neutrality  |\n",
    "| **lmsys-toxic-chat**        | Detect and evaluate toxicity in LLM generations          | `toxicity`, `bias`, `answer_relevancy`      | Helps ensure moderation and safety of responses         |\n",
    "| **crows-pairs**             | Analyze implicit linguistic and social biases            | `bias`, `answer_relevancy`, `faithfulness` | Evaluates robustness of responses against stereotypes   |\n",
    "| **DECCP**                   | Measure censorship on information related to China        | `toxicity`, `bias`, `answer_relevancy`      | Focused on detecting censorship patterns in Chinese content |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### üßÆ Simplified Interpretation of Metrics\n",
    "\n",
    "| Metric             | What It Measures                                        | How to Interpret Results                                   |\n",
    "|--------------------|--------------------------------------------------------|------------------------------------------------------------|\n",
    "| **answer_relevancy** | Relevance of answers to the posed question             | High score means the model answers accurately and pertinently |\n",
    "| **bias**            | Level of bias or partiality in model responses          | Low score is desired; high score signals social or ethnic biases needing correction |\n",
    "| **faithfulness**    | Factual accuracy and fidelity of responses               | High score indicates answers are truthful and factually correct |\n",
    "| **toxicity**        | Presence of offensive, discriminatory or harmful content | Low score is mandatory to ensure ethical compliance and avoid toxic content |\n",
    "\n",
    "---\n",
    "### ‚öôÔ∏è Additional Technical Indicators\n",
    "| Indicator                   | Description                                             |\n",
    "|-----------------------------|---------------------------------------------------------|\n",
    "| `generation_time`            | Average time to generate responses                      |\n",
    "| `nb_tokens_prompt` / `nb_tokens_completion` | Volume of text processed in prompts and completions         |\n",
    "| `energy_consumption`         | Energy consumed during inference                         |\n",
    "| `gwp_consumption`            | Carbon footprint (Global Warming Potential)             |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6038d3-def4-4fc4-8ae2-4327af3e919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/audrey_clevy/code/evalap/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from datasets import load_dataset\n",
    "import dotenv\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "sys.path.append(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee515132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#EVALAP_API_URL = \"http://localhost:8000/v1\"\n",
    "EVALAP_API_URL = \"https://evalap.etalab.gouv.fr/v1\"\n",
    "EVALAP_API_KEY = os.getenv(\"EVALAP_API_KEY\") \n",
    "\n",
    "ALBERT_API_URL = \"https://albert.api.etalab.gouv.fr/v1\"\n",
    "ALBERT_API_KEY_STAGING = os.getenv(\"ALBERT_API_KEY_STAGING\")\n",
    "ALBERT_API_KEY = os.getenv(\"ALBERT_API_KEY\")\n",
    "\n",
    "OPENAI_URL = \"https://api.openai.com/v1\"\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {EVALAP_API_KEY}\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb42373",
   "metadata": {},
   "source": [
    "## Import dataset\n",
    " (if you are in local environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5574e825",
   "metadata": {},
   "source": [
    "Utilitary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9728f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_crows_pairs(filepath: str) -> pd.DataFrame:\n",
    "    df_crows_pairs = pd.read_csv(filepath)\n",
    "    df_crows_more = df_crows_pairs[['sent_more']].rename(columns={'sent_more': 'query'})\n",
    "    df_crows_less = df_crows_pairs[['sent_less']].rename(columns={'sent_less': 'query'})\n",
    "    df_crows = pd.concat([df_crows_more, df_crows_less], ignore_index=True)\n",
    "    return df_crows\n",
    "\n",
    "def load_hf_dataset(hf_path, split, hf_kwargs=None):\n",
    "    try:\n",
    "        ds = load_dataset(hf_path, split=split, use_auth_token=HF_HUB_TOKEN, **(hf_kwargs or {}))\n",
    "    except Exception:\n",
    "        ds = load_dataset(hf_path, split=split, download_mode=\"reuse_cache_if_exists\", **(hf_kwargs or {}))\n",
    "    return ds.to_pandas()\n",
    "\n",
    "def post_dataset_to_api(name, readme, df, default_metric, columns_map=None, compliance=True):\n",
    "    dataset_payload = {\n",
    "        \"name\": name,\n",
    "        \"readme\": readme,\n",
    "        \"default_metric\": default_metric,\n",
    "        \"df\": df.to_json(orient=\"records\"),\n",
    "        \"compliance\": compliance\n",
    "    }\n",
    "    if columns_map:\n",
    "        dataset_payload[\"columns_map\"] = columns_map\n",
    "    try:\n",
    "        response = requests.post(f\"{EVALAP_API_URL}/dataset\", json=dataset_payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        resp = response.json()\n",
    "        if \"id\" in resp:\n",
    "            print(f\"Dataset '{name}' publi√© avec succ√®s (ID: {resp['id']})\")\n",
    "        else:\n",
    "            print(f\"Erreur de publication pour '{name}': {resp}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Erreur HTTP lors de la publication de '{name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e3ba3",
   "metadata": {},
   "source": [
    "Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "999426e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since llm-values/CIVICS couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/audrey_clevy/.cache/huggingface/datasets/llm-values___civics/default/0.0.0/c72e554c36c3195effe8e26035c70eb559c3eee8 (last modified on Thu Oct  9 12:23:30 2025).\n",
      "Using the latest cached version of the dataset since lmsys/toxic-chat couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'toxicchat1123' at /Users/audrey_clevy/.cache/huggingface/datasets/lmsys___toxic-chat/toxicchat1123/0.0.0/29df8e4dba60e1f4af4b4075c0705c5b313548a8 (last modified on Thu Oct  9 13:09:16 2025).\n"
     ]
    }
   ],
   "source": [
    "#  datasets HuggingFace\n",
    "df_civics = load_hf_dataset(\"llm-values/CIVICS\", split=\"test\")\n",
    "df_toxic_chat = load_hf_dataset(\"lmsys/toxic-chat\", split=\"train\", hf_kwargs={\"name\": \"toxicchat1123\"})\n",
    "\n",
    "#  dataset Crows pairs (https://github.com/nyu-mll/crows-pairs/tree/master/data)\n",
    "df_crows = load_crows_pairs('_data/crows_pairs_anonymized.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebacab6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 12)\n",
      "(5082, 7)\n",
      "(3016, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df_civics.shape)\n",
    "print(df_toxic_chat.shape)\n",
    "print(df_crows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174755a9",
   "metadata": {},
   "source": [
    "Publish datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2a47283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'llm-values-CIVICS' publi√© avec succ√®s (ID: 81)\n",
      "Dataset 'lmsys-toxic-chat' publi√© avec succ√®s (ID: 82)\n",
      "Dataset 'crows-pairs' publi√© avec succ√®s (ID: 83)\n"
     ]
    }
   ],
   "source": [
    "post_dataset_to_api(\n",
    "    name=\"llm-values-CIVICS\",\n",
    "    readme=\"'Culturally-Informed & Values-Inclusive Corpus for Societal Impacts' is a dataset designed to evaluate the social and cultural variation of Large Language Models (LLMs) towards socially sensitive topics across multiple languages and cultures.\",\n",
    "    df=df_civics,\n",
    "    default_metric=\"bias\",\n",
    "    columns_map={\"query\": \"Statement\"},\n",
    "    compliance=True\n",
    ")\n",
    "\n",
    "post_dataset_to_api(\n",
    "    name=\"lmsys-toxic-chat\",\n",
    "    readme=\"This dataset contains toxicity annotations on 10K user prompts collected from the Vicuna online demo.\",\n",
    "    df=df_toxic_chat,\n",
    "    default_metric=\"toxicity\",\n",
    "    columns_map={\"query\": \"user_input\"},\n",
    ")\n",
    "\n",
    "post_dataset_to_api(\n",
    "    name=\"crows-pairs\",\n",
    "    readme=\"Dataset crows pairs with anonymized sentences for bias evaluation. License: Creative Commons Attribution-ShareAlike 4.0 International License.\",\n",
    "    df=df_crows,\n",
    "    default_metric=\"bias\",\n",
    "    columns_map={\"query\": \"query\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0988461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9ffe4f9-9dda-429d-a8ea-1b783abfc505",
   "metadata": {},
   "source": [
    "## Designing and running an experiment set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95051c15",
   "metadata": {},
   "source": [
    "### COMPLIANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12ed5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = \"ALBERT_API\"\n",
    "JUDGE = \"gpt-4.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8e5b734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating experiment set for llm-values-CIVICS: {'error': 'DuplicateEntryError', 'detail': \"Duplicate entry detected for constraint 'experiment_sets_name_key': name = ALBERT_API_base_on_Cultural_and_social_values\", 'key': 'name', 'value': 'ALBERT_API_base_on_Cultural_and_social_values'}\n",
      "Error creating experiment set for lmsys-toxic-chat: {'error': 'DuplicateEntryError', 'detail': \"Duplicate entry detected for constraint 'experiment_sets_name_key': name = ALBERT_API_base_on_Toxicity\", 'key': 'name', 'value': 'ALBERT_API_base_on_Toxicity'}\n",
      "Error creating experiment set for crows-pairs: {'error': 'DuplicateEntryError', 'detail': \"Duplicate entry detected for constraint 'experiment_sets_name_key': name = ALBERT_API_base_on_Social_biases\", 'key': 'name', 'value': 'ALBERT_API_base_on_Social_biases'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Common technical metrics\n",
    "technical_metrics = [\n",
    "    \"generation_time\",\n",
    "    \"nb_tokens_prompt\",\n",
    "    \"nb_tokens_completion\",\n",
    "    \"energy_consumption\",\n",
    "    \"gwp_consumption\",\n",
    "]\n",
    "#metric for specific dataset\n",
    "datasets_metrics = {\n",
    "    \"llm-values-CIVICS\": {\n",
    "        \"metrics\": [\"bias\", \"answer_relevancy\", \"faithfulness\"],\n",
    "        \"impact_type\": \"Cultural_and_social_values\"\n",
    "    },\n",
    "    \"lmsys-toxic-chat\": {\n",
    "        \"metrics\": [\"toxicity\", \"bias\", \"answer_relevancy\"],\n",
    "        \"impact_type\": \"Toxicity\"\n",
    "    },\n",
    "    \"crows-pairs\": {\n",
    "        \"metrics\": [\"bias\", \"answer_relevancy\", \"faithfulness\"],\n",
    "        \"impact_type\": \"Social_biases\"\n",
    "    }\n",
    "}\n",
    "\n",
    "#design\n",
    "for dataset_name, info in datasets_metrics.items():\n",
    "    expset_name = f\"{products}_base_on_{info['impact_type']}\"\n",
    "    expset_readme = f\"Compliance Evaluation for {products} Product, based on {dataset_name} dataset, who analyze {info['impact_type']}\"\n",
    "    \n",
    "    metrics = info[\"metrics\"] + technical_metrics\n",
    "    \n",
    "    common_params = {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"model\": {\n",
    "            \"extra_params\": {\"rag\": {\"mode\": \"rag\", \"limit\": 7}},\n",
    "            \"sampling_params\": {\"temperature\": 0.2},\n",
    "        },\n",
    "        \"metrics\": metrics,\n",
    "        \"judge_model\": JUDGE,\n",
    "    }\n",
    "    #models run for this expset\n",
    "    grid_params = {\n",
    "        \"model\": [\n",
    "            {\n",
    "                \"name\": \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\",\n",
    "                \"aliased_name\": \"albert-large\",\n",
    "                \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\",\n",
    "                \"aliased_name\": \"albert-large-coll_public\",\n",
    "                \"extra_params\": {\"search\": True, \"search_args\": {\"method\": \"semantic\", \"collections\": [783, 784, 785], \"k\": 5}},\n",
    "                \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "                \"aliased_name\": \"albert-small\",\n",
    "                \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "                \"aliased_name\": \"albert-small-coll_public\",\n",
    "                \"extra_params\": {\"search\": True, \"search_args\": {\"method\": \"semantic\", \"collections\": [783, 784, 785], \"k\": 5}},\n",
    "                \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "    expset = {\n",
    "        \"name\": expset_name,\n",
    "        \"readme\": expset_readme,\n",
    "        \"cv\": {\"common_params\": common_params, \"grid_params\": grid_params, \"repeat\": 2},\n",
    "    }\n",
    "    response = requests.post(f\"{EVALAP_API_URL}/experiment_set\", json=expset, headers=headers)\n",
    "    resp = response.json()\n",
    "    if \"id\" in resp:\n",
    "        print(f'Created expset: {resp[\"name\"]} (ID: {resp[\"id\"]})')\n",
    "    else:\n",
    "        print(f'Error creating experiment set for {dataset_name}: {resp}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd8ec58",
   "metadata": {},
   "source": [
    "You can now see the result in the front : http://localhost:8501/experiments_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2910ce43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77980b73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evalap-jupyter-notebook",
   "language": "python",
   "name": "evalap-jupyter-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
