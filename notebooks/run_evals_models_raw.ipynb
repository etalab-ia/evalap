{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ecbc85-47c2-454d-90c8-71d415f83288",
   "metadata": {},
   "source": [
    "# Abert Model Evals \n",
    "\n",
    "Raw models evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6038d3-def4-4fc4-8ae2-4327af3e919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import dotenv\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "EVALAP_API_URL = \"http://localhost:8000/v1\"\n",
    "ALBERT_API_URL = \"https://albert.api.etalab.gouv.fr/v1\"\n",
    "# keys\n",
    "EVALAP_API_KEY = os.getenv(\"EVALAP_API_KEY\")\n",
    "ALBERT_API_KEY = os.getenv(\"ALBERT_API_KEY\")\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {EVALAP_API_KEY}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee953b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_name = \"gpt-4.1\"\n",
    "judge_api_url = \"https://api.openai.com/v1\"\n",
    "judge_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60dd8b-e7b5-4a08-86c7-1025c5b80f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design and run the experiments\n",
    "# --\n",
    "for dataset in [\"MFS_questions_v01\",  \"Assistant IA - QA\"]:\n",
    "    expset_name = f\"Albert - brut - ({dataset}) v1-10-25\"\n",
    "    expset_readme = f\"Comparing raw Albert models on dataset {dataset}\"\n",
    "    metrics = [\"judge_exactness\", \"judge_precision\", \"output_length\", \"generation_time\"]\n",
    "    common_params = {\n",
    "        \"dataset\" : dataset,\n",
    "        \"model\": {\"sampling_params\" : {\"temperature\": 0.2}},\n",
    "        \"metrics\" : metrics,\n",
    "        \"judge_model\": {\n",
    "            \"name\": judge_name,\n",
    "            \"base_url\": judge_api_url,\n",
    "            \"api_key\": judge_api_key,\n",
    "    },\n",
    "    }\n",
    "    grid_params = {\n",
    "        \"model\": [\n",
    "            {\"name\": \"meta-llama/Llama-3.1-8B-Instruct\", \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY},\n",
    "            {\"name\": \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\", \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY},\n",
    "            {\"name\": \"Qwen/Qwen3-30B-A3B-Instruct-2507\", \"base_url\": \"http://51.15.199.237/v1\", \"api_key\": \"changeme\"},\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Lauching the experiment set\n",
    "    expset = {\n",
    "        \"name\" : expset_name,\n",
    "        \"readme\": expset_readme,\n",
    "        \"cv\": {\"common_params\": common_params, \"grid_params\": grid_params, \"repeat\":2}\n",
    "    }\n",
    "    response = requests.post(f'{EVALAP_API_URL}/experiment_set', json=expset, headers=headers)\n",
    "    resp = response.json()\n",
    "    if \"id\" in resp:\n",
    "        expset_id = resp[\"id\"]\n",
    "        print(f'Created expset: {resp[\"name\"]} ({resp[\"id\"]})')\n",
    "    else:\n",
    "        print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21aefc-de8b-4436-af6f-4bc6323807a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATCH my experiments\n",
    "# --\n",
    "for dataset, expid in [(\"MFS_questions_v01\",73),  (\"Assistant IA - QA\",74)]:\n",
    "    expset_name = f\"Albert ({dataset}) v1-10-25\"\n",
    "    expset_readme = f\"Comparing Albert Models on dataset {dataset}\"\n",
    "    metrics = [\"judge_exactness\", \"judge_precision\", \"output_length\", \"generation_time\"]\n",
    "    common_params = {\n",
    "        \"dataset\" : dataset,\n",
    "        #\"model\": {\"sampling_params\" : {\"temperature\": 0.2}},\n",
    "        \"metrics\" : metrics,\n",
    "        \"judge_model\": \"gpt-4.1\",\n",
    "    }\n",
    "    grid_params = {\n",
    "        \"model\": [\n",
    "            #{\"name\":\"gpt-5\", \"base_url\":\"https://api.openai.com/v1\", \"api_key\":os.getenv(\"OPENAI_API_KEY\")}\n",
    "            {\"name\": \"mistralai/Magistral-Small-2506\", \"base_url\": \"http://51.15.199.237/v1\", \"api_key\": \"changeme\"},\n",
    "\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Lauching the experiment set\n",
    "    expset = {\n",
    "        \"name\" : expset_name,\n",
    "        \"readme\": expset_readme,\n",
    "        \"cv\": {\"common_params\": common_params, \"grid_params\": grid_params, \"repeat\":2}\n",
    "    }\n",
    "    response = requests.patch(f'{EVALAP_API_URL}/experiment_set/{expid}', json=expset, headers=headers)\n",
    "    resp = response.json()\n",
    "    if \"id\" in resp:\n",
    "        expset_id = resp[\"id\"]\n",
    "        print(f'Patched expset: {resp[\"name\"]} ({resp[\"id\"]})')\n",
    "    else:\n",
    "        print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6156a8a-6812-49e7-9c22-3d8df99853b0",
   "metadata": {},
   "source": [
    "## Reading and showing results\n",
    "\n",
    "-> Show the mean and std score, for each metrics, across the experiment repetition. The std show the variability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbaf493-b597-4ae2-a9eb-66b43b3afe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various utility functions\n",
    "# --\n",
    "def format_metrics(row):\n",
    "   # format a dataframe has a series of \"mean ± std\"\n",
    "   metrics = {}\n",
    "   for metric in final_df.columns.levels[0]:\n",
    "       mean_value = row[(metric, 'mean')]\n",
    "       std_value = row[(metric, 'std')]\n",
    "       metrics[metric] = f\"{mean_value:.2f} ± {std_value:.2f}\"\n",
    "   return pd.Series(metrics)\n",
    "\n",
    "def highlight_cells(s):\n",
    " # Custom function to highlight the entry with the highest/lowest mean value\n",
    "    means = s.apply(lambda x: float(x.split('±')[0].strip()))\n",
    "    # Create a mask where 1 for max, 0 for min\n",
    "    max_mean_index = means.idxmax()\n",
    "    min_mean_index = means.idxmin()\n",
    "    mask = pd.Series({max_mean_index: 1, min_mean_index: 0}, index=s.index)\n",
    "    return [\n",
    "        'font-weight: bold; color: salmon' if mask_value == 0 else\n",
    "        'font-weight: bold; color: green' if mask_value == 1 else\n",
    "        ''\n",
    "        for mask_value in mask\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177816f4-1105-497d-a14b-7178850d36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read results\n",
    "# --\n",
    "df_all = None # multi-dimensional DataFrame\n",
    "arr_all = {} # keep references of source array per metric metrics\n",
    "\n",
    "# Fetch results and compute macro metrics (mean, std etc).\n",
    "# --\n",
    "response = requests.get(f'{EVALAP_API_URL}/experiment_set/{expset_id}', headers=headers)\n",
    "expset = response.json()\n",
    "rows = []\n",
    "for i, exp in enumerate(expset[\"experiments\"]):\n",
    "    # Get an experiment result\n",
    "    exp_id = exp[\"id\"]\n",
    "    response = requests.get(f'{EVALAP_API_URL}/experiment/{exp_id}?with_results=true', headers=headers)\n",
    "    experiment = response.json()\n",
    "    # experiment[\"name\"] # Name of the experiment\n",
    "    if experiment[\"experiment_status\"] != \"finished\":\n",
    "        print(f\"Warning: experiment {exp_id} is not finished yet...\")\n",
    "    results = experiment[\"results\"]\n",
    "    model = experiment[\"model\"][\"name\"]\n",
    "    if experiment[\"model\"].get(\"extra_params\"):\n",
    "        if experiment[\"model\"][\"extra_params\"].get(\"rag\") :\n",
    "            model = experiment[\"model\"]['name'] + \"--rag\"\n",
    "\n",
    "    # Add an observation row from the observation_table (mean, std etc)\n",
    "    row = {\"model\": model}\n",
    "    rows.append(row)\n",
    "    metric_arrs = {}\n",
    "    arr_all[model] = metric_arrs\n",
    "    for metric_results in results:\n",
    "        metric = metric_results[\"metric_name\"]\n",
    "        arr = np.array([x[\"score\"] for x in metric_results[\"observation_table\"] if pd.notna(x[\"score\"])])\n",
    "        row[(metric, 'mean')] = np.mean(arr)\n",
    "        row[(metric, 'std')] = np.std(arr)\n",
    "        row[(metric, 'median')] = np.median(arr)\n",
    "        row[(metric, 'mean_std')] = f\"{arr.mean():.2f} ± {arr.std():.2f}\"  # Formatting as 'mean±std'\n",
    "        row[(metric, 'support')] = len(arr)\n",
    "        metric_arrs[metric] = arr\n",
    "\n",
    "df_all = pd.DataFrame(rows)\n",
    "df_all.set_index('model', inplace=True)\n",
    "df_all.columns = pd.MultiIndex.from_tuples(df_all.columns)\n",
    "final_df = df_all.xs('mean', axis=1, level=1) # pick the \"macro\" metric to show (mean, std, support etc)\n",
    "\n",
    "# Group and average the result of the experiments by models\n",
    "# --\n",
    "final_df = final_df.groupby(level=0).agg(['mean', 'std'])  # groupby \"model\"\n",
    "final_df.index.name = None\n",
    "final_df = final_df.apply(format_metrics, axis=1) # final formating\n",
    "\n",
    "#final_df = final_df.reindex([m[\"name\"] for m in grid_params[\"model\"]]) # reorder rows\n",
    "final_df = final_df.sort_values(by='judge_exactness', ascending=False)\n",
    "final_df = final_df[metrics] # reorder columns\n",
    "final_df = final_df.style.apply(highlight_cells, axis=0)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f27987a-d0c1-46d3-949e-407c3aa1895f",
   "metadata": {},
   "source": [
    "# Visualize the score dispersion for each metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff216c-0ea4-48b5-a71b-dc980313aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for model, metrics_arr in arr_all.items():\n",
    "    experiment_title = model\n",
    "    metrics_arr = {k:metrics_arr[k] for k in metrics} # reorder metrics\n",
    "    # Pad missing value with np.nan\n",
    "    arrays = list(metrics_arr.values())\n",
    "    max_length = max(len(arr) for arr in arrays)\n",
    "    arr_metrics = np.array([np.pad(arr, (0, max_length - len(arr)), constant_values=np.nan) for arr in arrays])\n",
    "    df = pd.DataFrame(arr_metrics.T, columns=metrics_arr.keys())\n",
    "\n",
    "    num_cols = 5\n",
    "    num_plots = len(df.columns)\n",
    "    num_rows = (num_plots + num_cols - 1) // num_cols\n",
    "    plt.figure(figsize=(num_cols * 4, num_rows * 4))\n",
    "    for i, column in enumerate(df.columns, 1):\n",
    "        plt.subplot(num_rows, num_cols, i)\n",
    "        sns.histplot(df[column], bins=20, stat=\"probability\", element=\"bars\", kde=True)\n",
    "        plt.ylabel(\"\")\n",
    "\n",
    "    plt.suptitle(experiment_title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c86d1-18b8-4ba8-b827-9382a26beda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRY failed experiment !\n",
    "# --\n",
    "response = requests.post(f'{EVALAP_API_URL}/retry/experiment_set/{expset_id}', headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0d32b4-b903-45e9-8add-51e4c8b91677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
