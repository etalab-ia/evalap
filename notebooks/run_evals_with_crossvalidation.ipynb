{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ecbc85-47c2-454d-90c8-71d415f83288",
   "metadata": {},
   "source": [
    "# Demo for launching an experiment set with crossvalidation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6038d3-def4-4fc4-8ae2-4327af3e919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import dotenv\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "EVALAP_API_URL = \"http://localhost:8000/v1\"\n",
    "#EVALAP_API_URL = \"https://evalap.etalab.gouv.fr/v1\"\n",
    "EVALAP_API_KEY = os.getenv(\"EVALAP_API_KEY\") \n",
    "ALBERT_API_URL = \"https://albert.api.etalab.gouv.fr/v1\"\n",
    "ALBERT_API_KEY = os.getenv(\"ALBERT_API_KEY\")\n",
    "MFS_API_URL = \"https://franceservices.etalab.gouv.fr/api/v1\"\n",
    "MFS_API_KEY = os.getenv(\"MFS_API_KEY\")\n",
    "headers = {\"Authorization\": f\"Bearer {EVALAP_API_KEY}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e7ad4-defb-4399-b70b-b63331672438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various utility functions\n",
    "# --\n",
    "def format_metrics(row):\n",
    "   # format a dataframe has a series of \"mean ± std\"\n",
    "   metrics = {}\n",
    "   for metric in final_df.columns.levels[0]:\n",
    "       mean_value = row[(metric, 'mean')]\n",
    "       std_value = row[(metric, 'std')]\n",
    "       metrics[metric] = f\"{mean_value:.2f} ± {std_value:.2f}\"\n",
    "   return pd.Series(metrics)\n",
    "    \n",
    "def highlight_cells(s):\n",
    " # Custom function to highlight the entry with the highest/lowest mean value\n",
    "    means = s.apply(lambda x: float(str(x).split('±')[0].strip()))\n",
    "    # Create a mask where 1 for max, 0 for min\n",
    "    max_mean_index = means.idxmax()\n",
    "    min_mean_index = means.idxmin()  \n",
    "    mask = pd.Series({max_mean_index: 1, min_mean_index: 0}, index=s.index)\n",
    "    return [\n",
    "        'font-weight: bold; color: salmon' if mask_value == 0 else\n",
    "        'font-weight: bold; color: green' if mask_value == 1 else\n",
    "        ''\n",
    "        for mask_value in mask\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ffe4f9-9dda-429d-a8ea-1b783abfc505",
   "metadata": {},
   "source": [
    "## Designing and running an experiment set with raglimit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1a4a1",
   "metadata": {},
   "source": [
    "*Objectif*: **Comparing the impact of the `limit` parameters on a RAG model**\n",
    "\n",
    "An experiment set is a collection of experiments that are part of the same evaluation scenario. \n",
    "In this notebook, we're comparing how the (maximum) number of chunks influences the model's performance.\n",
    "\n",
    "To conduct these experiments, one approach consists by creating an empty experiment set (via POST /experiment_set) and then add a list of experiments to it (via POST /experiment with the reference to the experimentset_id). Each experiment should have all parameters the same, except for the (maximum) number of chunks.\n",
    "\n",
    "Alternatively, the /experiment_set endpoint offers a convenient feature called cv (short for cross-validation). This feature includes two key parameters:\n",
    "\n",
    "- **common_params**: These are the parameters that will be shared across all experiments in the set.\n",
    "- **grid_params**: This allows you to specify a list of varying values for any parameter.\n",
    "\n",
    "\n",
    "Both **commons_params** and **grid_params** accept all the parameter defined by the ExperimentSetCreate schema.  \n",
    "The experiments will be generated by combining the **common_params** with each unique set of values from the cartesian product of the lists provided in **grid_params**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b45cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_name = \"gpt-4.1\"\n",
    "judge_api_url = \"https://api.openai.com/v1\",\n",
    "judge_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60dd8b-e7b5-4a08-86c7-1025c5b80f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designing my experiments\n",
    "# --\n",
    "expset_name = \"mfs_rag_limit_v1\"\n",
    "expset_readme = \"Comparing the impact of the `limit` parameters on a RAG model.\"\n",
    "metrics = [\"answer_relevancy\", \"judge_exactness\", \"judge_notator\", \"output_length\", \"generation_time\"]\n",
    "common_params = {\n",
    "    \"dataset\" : \"MFS_questions_v01\",\n",
    "    \"model\" : {\"name\": \"meta-llama/Llama-3.1-8B-Instruct\", \"sampling_params\": {\"temperature\": 0.2}, \"base_url\": MFS_API_URL, \"api_key\": MFS_API_KEY},\n",
    "    \"metrics\" : metrics,\n",
    "    \"judge_model\": {\n",
    "        \"name\": judge_name,\n",
    "        \"base_url\": judge_api_url,\n",
    "        \"api_key\": judge_api_key,\n",
    "    },\n",
    "}\n",
    "grid_params = {\n",
    "    \"model\": [{\"extra_params\": {\"rag\": {\"mode\":\"rag\", \"limit\":i}}} for i in [1, 2, 3, 4, 5, 7, 10, 15, 20]],\n",
    "}\n",
    "\n",
    "# Lauching the experiment set\n",
    "expset = {\n",
    "    \"name\" : expset_name, \n",
    "    \"readme\": expset_readme,\n",
    "    \"cv\": {\"common_params\": common_params, \"grid_params\": grid_params}\n",
    "}\n",
    "response = requests.post(f'{EVALAP_API_URL}/experiment_set', json=expset, headers=headers)\n",
    "resp = response.json()\n",
    "if \"id\" in resp:\n",
    "    expset_id = resp[\"id\"]\n",
    "    print(f'Created expset: {resp[\"name\"]} ({resp[\"id\"]})')\n",
    "else:\n",
    "    print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a46a4",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d9ad6",
   "metadata": {},
   "source": [
    "## Designing and running an experiment set with repeat params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8511df4",
   "metadata": {},
   "source": [
    "*Objectif*: **Comparing some models variability**\n",
    "\n",
    "An experiment set is a collection of experiments that are part of the same evaluation scenario. \n",
    "In this notebook, we're comparing how the (maximum) number of chunks influences the model's performance.\n",
    "\n",
    "To conduct these experiments, one approach consists by creating an empty experiment set (via POST /experiment_set) and then add a list of experiments to it (via POST /experiment with the reference to the experimentset_id). Each experiment should have all parameters the same, except for the (maximum) number of chunks.\n",
    "\n",
    "Alternatively, the /experiment_set endpoint offers a convenient feature called cv (short for cross-validation). This feature includes two key parameters:\n",
    "\n",
    "- **common_params**: These are the parameters that will be shared across all experiments in the set.\n",
    "- **grid_params**: This allows you to specify a list of varying values for any parameter.\n",
    "\n",
    "\n",
    "Both **commons_params** and **grid_params** accept all the parameter defined by the ExperimentSetCreate schema.  \n",
    "The experiments will be generated by combining the **common_params** with each unique set of values from the cartesian product of the lists provided in **grid_params**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d7066",
   "metadata": {},
   "source": [
    "# Designing my experiments\n",
    "# --\n",
    "expset_name = \"mfs_variability_v3\"\n",
    "expset_readme = \"Comparing some models variability.\"\n",
    "metrics = [\"answer_relevancy\", \"judge_exactness\", \"judge_notator\", \"output_length\", \"generation_time\"]\n",
    "common_params = {\n",
    "    \"dataset\" : \"MFS_questions_v01\",\n",
    "    \"model\": {\"sampling_params\" : {\"temperature\": 0.2}},\n",
    "    \"metrics\" : metrics,\n",
    "    \"judge_model\": \"gpt-4o\",\n",
    "}\n",
    "grid_params = {\n",
    "    \"model\": [\n",
    "        {\"name\": \"google/gemma-2-9b-it\",                   \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY},\n",
    "        {\"name\": \"meta-llama/Llama-3.1-8B-Instruct\",       \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY},\n",
    "        {\"name\": \"meta-llama/Llama-3.2-3B-Instruct\",  \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY},\n",
    "        {\"name\": \"neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8\",  \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY},\n",
    "        {\"name\": \"meta-llama/Llama-3.3-70B-Instruct\", \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY},\n",
    "        {\"name\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\", \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY},\n",
    "        {\n",
    "          \"name\": \"meta-llama/Llama-3.1-8B-Instruct\", \"extra_params\": {\"rag\": {\"mode\":\"rag\", \"limit\":7}}, \n",
    "          \"base_url\": MFS_API_URL, \"api_key\": MFS_API_KEY, \n",
    "        },\n",
    "        {\n",
    "          \"name\": \"AgentPublic/llama3-instruct-guillaumetell\", \"extra_params\": {\"rag\": {\"mode\":\"rag\", \"limit\":7}}, \n",
    "          \"base_url\": MFS_API_URL, \"api_key\": MFS_API_KEY, \n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Lauching the experiment set\n",
    "expset = {\n",
    "    \"name\" : expset_name, \n",
    "    \"readme\": expset_readme,\n",
    "    \"cv\": {\"common_params\": common_params, \"grid_params\": grid_params, \"repeat\":5}\n",
    "}\n",
    "response = requests.post(f'{EVALAP_API_URL}/experiment_set', json=expset, headers=headers)\n",
    "resp = response.json()\n",
    "if \"id\" in resp:\n",
    "    expset_id = resp[\"id\"]\n",
    "    print(f'Created expset: {resp[\"name\"]} ({resp[\"id\"]})')\n",
    "else:\n",
    "    print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69abe70d",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13931dff",
   "metadata": {},
   "source": [
    "## Designing and running an experiment set with rag metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3236199",
   "metadata": {},
   "source": [
    "\n",
    "*Objectif*: **Comparing baseline models with RAG metrics (use the _retriever_context_)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designing my experiments\n",
    "# --\n",
    "expset_name = \"mfs_RAG_metrics_v3\"\n",
    "expset_readme = \"Evaluation of baseline models with RAG metrics (retriever_context).\"\n",
    "metrics = [\"contextual_relevancy\", \"contextual_recall\", \"contextual_precision\"]\n",
    "common_params = {\n",
    "    \"dataset\" : \"MFS_questions_v01\",\n",
    "    \"model\": {\n",
    "        \"extra_params\": {\"rag\": {\"mode\":\"rag\", \"limit\":7}},\n",
    "        \"sampling_params\" : {\"temperature\": 0.2}\n",
    "    },\n",
    "    \"metrics\" : metrics,\n",
    "    \"judge_model\": \"gpt-4o\",\n",
    "}\n",
    "grid_params = {\n",
    "    \"model\": [\n",
    "        {\n",
    "          \"name\": \"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "          \"base_url\": MFS_API_URL, \"api_key\": MFS_API_KEY, \n",
    "        },\n",
    "        {\n",
    "          \"name\": \"google/gemma-2-9b-it\", \n",
    "          \"base_url\": MFS_API_URL, \"api_key\": MFS_API_KEY, \n",
    "        },\n",
    "        {\n",
    "          \"name\": \"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "          \"base_url\": MFS_API_URL, \"api_key\": MFS_API_KEY, \n",
    "        },\n",
    "        {\n",
    "          \"name\": \"AgentPublic/llama3-instruct-guillaumetell\", \n",
    "          \"base_url\": MFS_API_URL, \"api_key\": MFS_API_KEY, \n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Lauching the experiment set\n",
    "expset = {\n",
    "    \"name\" : expset_name, \n",
    "    \"readme\": expset_readme,\n",
    "    \"cv\": {\"common_params\": common_params, \"grid_params\": grid_params, \"repeat\":3}\n",
    "}\n",
    "response = requests.post(f'{EVALAP_API_URL}/experiment_set', json=expset, headers=headers)\n",
    "resp = response.json()\n",
    "if \"id\" in resp:\n",
    "    expset_id = resp[\"id\"]\n",
    "    print(f'Created expset: {resp[\"name\"]} ({resp[\"id\"]})')\n",
    "else:\n",
    "    print(resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
