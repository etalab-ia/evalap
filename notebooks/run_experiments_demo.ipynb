{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ecbc85-47c2-454d-90c8-71d415f83288",
   "metadata": {},
   "source": [
    "# Demo for launching a simple experiment in evalap\n",
    "\n",
    "*Objectif*: **Comparing some baseline models on a few metrics**\n",
    "\n",
    "- code: https://github.com/etalab-ia/evalap\n",
    "- api documentation: https://evalap.etalab.gouv.fr/redoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb6038d3-def4-4fc4-8ae2-4327af3e919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import dotenv\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "sys.path.append(\"..\")\n",
    "from evalap.utils import log_and_raise_for_status\n",
    "\n",
    "#EVALAP_API_URL = \"http://localhost:8000/v1\"\n",
    "EVALAP_API_URL = \"https://evalap.etalab.gouv.fr/v1\"\n",
    "EVALAP_API_KEY = os.getenv(\"EVALAP_API_KEY\") \n",
    "ALBERT_API_URL = \"https://albert.api.etalab.gouv.fr/v1\"\n",
    "ALBERT_API_KEY = os.getenv(\"ALBERT_API_KEY\")\n",
    "MFS_API_URL = \"https://franceservices.etalab.gouv.fr/api/v1\"\n",
    "MFS_API_KEY = os.getenv(\"MFS_API_KEY\")\n",
    "headers = {\"Authorization\": f\"Bearer {EVALAP_API_KEY}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf3c78f-2b5f-4b9c-bb02-101460395bbf",
   "metadata": {},
   "source": [
    "## Load a dataset on which your evaluation is grounded\n",
    "\n",
    "evalap can used only 3 fields (more soon): \n",
    "- query: the model input question for LLM.\n",
    "- output(optional): a answer to the question pre-generated by the user.\n",
    "- ouptut_true(optional): the ground truth answer.\n",
    "\n",
    "-> Each metric has a set of required input in their specidication. The metrics used in an experiment will therefore constraint what field is waited in the dataset you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f466a3ee-9b1a-4115-8d71-94297d3ced2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>output_true</th>\n",
       "      <th>top_valid</th>\n",
       "      <th>operateur</th>\n",
       "      <th>thematique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comment contester un PV reçu depuis l’Italie ?</td>\n",
       "      <td>Pour contester un PV reçu depuis l'Italie, il ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ANTS</td>\n",
       "      <td>Amendes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quelles aides judiciaires trouver en France po...</td>\n",
       "      <td>Vous pouvez bénéficier de l'aide juridictionne...</td>\n",
       "      <td>0</td>\n",
       "      <td>ANTS</td>\n",
       "      <td>Amendes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Une mairie peut-elle refuser un formulaire cer...</td>\n",
       "      <td>Non, une mairie ne peut pas refuser un formula...</td>\n",
       "      <td>1</td>\n",
       "      <td>ANTS</td>\n",
       "      <td>CNI/passeport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comment renouveler une Carte Nationale d’Ident...</td>\n",
       "      <td>Pour renouveler une Carte Nationale d'Identité...</td>\n",
       "      <td>1</td>\n",
       "      <td>ANTS</td>\n",
       "      <td>CNI/passeport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quel formulaire cerfa utiliser pour renouveler...</td>\n",
       "      <td>Si vous ne faites pas la pré-demande, vous dev...</td>\n",
       "      <td>1</td>\n",
       "      <td>ANTS</td>\n",
       "      <td>CNI/passeport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0     Comment contester un PV reçu depuis l’Italie ?   \n",
       "1  Quelles aides judiciaires trouver en France po...   \n",
       "2  Une mairie peut-elle refuser un formulaire cer...   \n",
       "3  Comment renouveler une Carte Nationale d’Ident...   \n",
       "4  Quel formulaire cerfa utiliser pour renouveler...   \n",
       "\n",
       "                                         output_true  top_valid operateur  \\\n",
       "0  Pour contester un PV reçu depuis l'Italie, il ...          1      ANTS   \n",
       "1  Vous pouvez bénéficier de l'aide juridictionne...          0      ANTS   \n",
       "2  Non, une mairie ne peut pas refuser un formula...          1      ANTS   \n",
       "3  Pour renouveler une Carte Nationale d'Identité...          1      ANTS   \n",
       "4  Si vous ne faites pas la pré-demande, vous dev...          1      ANTS   \n",
       "\n",
       "      thematique  \n",
       "0        Amendes  \n",
       "1        Amendes  \n",
       "2  CNI/passeport  \n",
       "3  CNI/passeport  \n",
       "4  CNI/passeport  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfs_dataset = pd.read_csv(\"_data/evaluation_set_qo_qcm.csv\", delimiter=\";\")\n",
    "mfs_dataset.rename(columns={'true_answer': 'output_true'}, inplace=True) # to be compliant with evalap\n",
    "mfs_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb2650-1175-43a4-9482-5d43801602b1",
   "metadata": {},
   "source": [
    "## Publish the dataset on evalap\n",
    "\n",
    "If the dataset already exists you'll geta DuplicateEntry error. That is normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "450399aa-f836-47b7-b6f6-2541da8a9f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'MFS_questions_v01',\n",
       " 'readme': 'MFS dataset with ground truth',\n",
       " 'default_metric': 'judge_notator',\n",
       " 'columns_map': None,\n",
       " 'id': 1,\n",
       " 'created_at': '2025-08-07T21:52:28.949956',\n",
       " 'size': 39,\n",
       " 'columns': ['query', 'output_true', 'top_valid', 'operateur', 'thematique'],\n",
       " 'parquet_size': 0,\n",
       " 'parquet_columns': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Publish a dataset\n",
    "dataset = {\"name\": \"MFS_questions_v01\", \"readme\": \"MFS dataset with ground truth\"\n",
    "           , \"default_metric\" : \"judge_notator\"\n",
    "           , \"df\": mfs_dataset.to_json()}\n",
    "response = requests.post(f'{EVALAP_API_URL}/dataset', json=dataset, headers=headers)\n",
    "resp = response.json()\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f703f8-8ef0-4015-b030-bce55ccf77e0",
   "metadata": {},
   "source": [
    "## List all avalaible metrics and their information\n",
    "\n",
    "- the `require` field indicates which fields is required in the dataset for this metrics to operate.\n",
    "- the `type` field is ignore for now. It will be associated with the type of the observation you get in the result output later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e461411-899a-4cec-a14d-4ad419b30c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>type</th>\n",
       "      <th>require</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>judge_complexity</td>\n",
       "      <td>[0-10] score complexity of query, thematic...</td>\n",
       "      <td>dataset</td>\n",
       "      <td>[query]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>see https://docs.confident-ai.com/docs/metrics-introduction</td>\n",
       "      <td>deepeval</td>\n",
       "      <td>[output, query]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bias</td>\n",
       "      <td>see https://docs.confident-ai.com/docs/metrics-introduction</td>\n",
       "      <td>deepeval</td>\n",
       "      <td>[output, query]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>contextual_precision</td>\n",
       "      <td>see https://docs.confident-ai.com/docs/metrics-introduction</td>\n",
       "      <td>deepeval</td>\n",
       "      <td>[output, output_true, query, retrieval_context]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>contextual_recall</td>\n",
       "      <td>see https://docs.confident-ai.com/docs/metrics-introduction</td>\n",
       "      <td>deepeval</td>\n",
       "      <td>[output, output_true, query, retrieval_context]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>contextual_relevancy</td>\n",
       "      <td>see https://docs.confident-ai.com/docs/metrics-introduction</td>\n",
       "      <td>deepeval</td>\n",
       "      <td>[output, query, retrieval_context]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>faithfulness</td>\n",
       "      <td>see https://docs.confident-ai.com/docs/metrics-introduction</td>\n",
       "      <td>deepeval</td>\n",
       "      <td>[output, query, retrieval_context]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hallucination</td>\n",
       "      <td>see https://docs.confident-ai.com/docs/metrics-introduction</td>\n",
       "      <td>deepeval</td>\n",
       "      <td>[context, output, query]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ragas</td>\n",
       "      <td>see https://docs.confident-ai.com/docs/metrics-introduction</td>\n",
       "      <td>deepeval</td>\n",
       "      <td>[output_true, query, retrieval_context]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>toxicity</td>\n",
       "      <td>see https://docs.confident-ai.com/docs/metrics-introduction</td>\n",
       "      <td>deepeval</td>\n",
       "      <td>[output, query]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>judge_completude</td>\n",
       "      <td>[0-100] score of the completude correspondance between output and output_true</td>\n",
       "      <td>llm</td>\n",
       "      <td>[output, output_true]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>judge_exactness</td>\n",
       "      <td>Binary similarity between output and output_true</td>\n",
       "      <td>llm</td>\n",
       "      <td>[output, output_true, query]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>judge_notator</td>\n",
       "      <td>[1-10] score semantic similarity between output and output_true</td>\n",
       "      <td>llm</td>\n",
       "      <td>[output, output_true, query]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>judge_precision</td>\n",
       "      <td>Binary precision of the output_true. Equal to one if the correct answer is contained in the given answer.</td>\n",
       "      <td>llm</td>\n",
       "      <td>[output, output_true, query]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>judge_rambling</td>\n",
       "      <td>[1-10] rambling score between output and output_true</td>\n",
       "      <td>llm</td>\n",
       "      <td>[output, output_true, query]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>judge_relevant</td>\n",
       "      <td>[1-10] relevance score between output and output_true</td>\n",
       "      <td>llm</td>\n",
       "      <td>[output, output_true, query]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>output_length</td>\n",
       "      <td>Number of words of the output</td>\n",
       "      <td>llm</td>\n",
       "      <td>[output]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>qcm_exactness</td>\n",
       "      <td>output and output_true binary equality</td>\n",
       "      <td>llm</td>\n",
       "      <td>[output, output_true]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ocr_v1</td>\n",
       "      <td>Levenshtein distance between the output and the ground-truth markdown.</td>\n",
       "      <td>ocr</td>\n",
       "      <td>[output, output_true]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>energy_consumption</td>\n",
       "      <td>Energy consumption (kWh) - Environmental impact calculated by librairy ecologits</td>\n",
       "      <td>ops</td>\n",
       "      <td>[output]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>generation_time</td>\n",
       "      <td>The time to generate the answer/output</td>\n",
       "      <td>ops</td>\n",
       "      <td>[output]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gwp_consumption</td>\n",
       "      <td>Global Warming Potential (kgCO2eq) - Environmental impact calculated by librairy ecologits</td>\n",
       "      <td>ops</td>\n",
       "      <td>[output]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nb_tokens_completion</td>\n",
       "      <td>Number of tokens in the completion</td>\n",
       "      <td>ops</td>\n",
       "      <td>[output]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nb_tokens_prompt</td>\n",
       "      <td>Number of tokens in the prompt</td>\n",
       "      <td>ops</td>\n",
       "      <td>[query]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nb_tool_calls</td>\n",
       "      <td>Number of tools that has been called for the generation</td>\n",
       "      <td>ops</td>\n",
       "      <td>[output]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show available metrics\n",
    "# - the require fields should be an existing field to the dataset used with the metric.\n",
    "# - output metric can be generated from the query (see below)\n",
    "response = requests.get(f'{EVALAP_API_URL}/metrics', headers=headers)\n",
    "all_metrics = response.json()\n",
    "df = pd.DataFrame(all_metrics).sort_values(by=[\"type\", \"name\"])\n",
    "HTML(df.to_html(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ffe4f9-9dda-429d-a8ea-1b783abfc505",
   "metadata": {},
   "source": [
    "## Lauching a couple of experiments\n",
    "\n",
    "Here we lauched the experiment independantly with the `/experiment` route. We will see on another notebook how to launch grouped experiments in a grid search fashion by using the route `/experiment_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b60dd8b-e7b5-4a08-86c7-1025c5b80f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created experiment: MFS_questions>AgentPublic/llama3-instruct-8b_0_v0 (25), status: running_answers\n",
      "Created experiment: MFS_questions>mfs-rag-baseline_1_v0 (26), status: running_answers\n"
     ]
    }
   ],
   "source": [
    "# Launch an experiment with a given **dataset**, **model** and set of **metrics** to compute.\n",
    "# - the model generate the \"output\" from the \"query\"\n",
    "# - you can also pass the \"output\" column instead of a model if you generated the answer by yourself...\n",
    "\n",
    "# Designing my experiments\n",
    "dataset = \"MFS_questions_v01\"\n",
    "models_to_test = [\n",
    "    {\"name\": \"meta-llama/Llama-3.1-8B-Instruct\",         \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY},\n",
    "    {\"name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",  \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY},\n",
    "    {\"name\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\", \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY},\n",
    "    {\n",
    "      \"_name\": \"mfs-rag-baseline\",\n",
    "      \"name\": \"AgentPublic/llama3-instruct-8b\", \"extra_params\": {\"rag\": {\"mode\":\"rag\", \"limit\":7}}, \n",
    "      \"base_url\": MFS_API_URL, \"api_key\": MFS_API_KEY, \n",
    "    },    \n",
    "]\n",
    "sampling_params = {\"temperature\": 0.2} # use the same sampling params for all in this evalation\n",
    "metrics = [\"output_length\", \"answer_relevancy\"]\n",
    "\n",
    "# Lauching the experiments\n",
    "experiment_ids = []\n",
    "for i, model in enumerate(models_to_test):\n",
    "    name = model[\"_name\"] if model.get(\"_name\") else model[\"name\"]\n",
    "    model[\"sampling_params\"] = sampling_params\n",
    "    model = model.copy()\n",
    "    model.pop(\"_name\") if \"_name\" in model else None\n",
    "    experiment = {\n",
    "        \"name\" : f\"MFS_questions>{name}_{i}_v0\", \n",
    "        \"dataset\": dataset,\n",
    "        \"model\": model,\n",
    "        \"metrics\": metrics,\n",
    "    }\n",
    "    response = requests.post(f'{EVALAP_API_URL}/experiment', json=experiment, headers=headers)\n",
    "    resp = response.json()\n",
    "    if \"id\" in resp:\n",
    "        experiment_ids.append(resp[\"id\"])\n",
    "        print(f'Created experiment: {resp[\"name\"]} ({resp[\"id\"]}), status: {resp[\"experiment_status\"]}')\n",
    "    else:\n",
    "        print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f40ad66f-2920-47d2-9cac-1edc51c863b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated experiment: MFS_questions>AgentPublic/llama3-instruct-8b_0_v0 (5), status: finished\n",
      "Updated experiment: MFS_questions>meta-llama/Meta-Llama-3.1-8B-Instruct_1_v0 (6), status: finished\n",
      "Updated experiment: MFS_questions>meta-llama/Meta-Llama-3.1-70B-Instruct_2_v0 (7), status: finished\n",
      "Updated experiment: MFS_questions>mfs-rag-baseline_3_v0 (8), status: finished\n"
     ]
    }
   ],
   "source": [
    "# Add or recompute a metric to an existing experiment(s)\n",
    "# - If you want to update one or many metriccs without relaucnhing the answer generation\n",
    "# -> In this exemple we add the generation_time metric to the list of computed metrics (generation time is kept in memory at inference time)\n",
    "for exp_id in experiment_ids:\n",
    "    experiment = {\n",
    "        \"metrics\": [\"generation_time\"],\n",
    "        \"rerun_answers\": False,\n",
    "    }\n",
    "    response = requests.patch(f'{EVALAP_API_URL}/experiment/{exp_id}', json=experiment, headers=headers)\n",
    "    resp = response.json()\n",
    "    if \"id\" in resp:\n",
    "        print(f'Updated experiment: {resp[\"name\"]} ({resp[\"id\"]}), status: {resp[\"experiment_status\"]}')\n",
    "    else:\n",
    "        print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6156a8a-6812-49e7-9c22-3d8df99853b0",
   "metadata": {},
   "source": [
    "## Reading and showing results\n",
    "\n",
    "-> The table in the ouput show the mean and std score, for each metrics, across the dataset questions. So it show the variability overs the dataset questios for one run, not the \"natural\" variability of the model (random variation across multiple generations). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "177816f4-1105-497d-a14b-7178850d36f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>judge_completude</th>\n",
       "      <th>judge_exactness</th>\n",
       "      <th>judge_notator</th>\n",
       "      <th>output_length</th>\n",
       "      <th>generation_time</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AgentPublic/llama3-instruct-8b</th>\n",
       "      <td>0.90 ± 0.23</td>\n",
       "      <td>26.41 ± 22.27</td>\n",
       "      <td>0.03 ± 0.16</td>\n",
       "      <td>3.41 ± 1.97</td>\n",
       "      <td>308.33 ± 93.23</td>\n",
       "      <td>6.64 ± 1.99</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta-llama/Meta-Llama-3.1-8B-Instruct</th>\n",
       "      <td>0.95 ± 0.15</td>\n",
       "      <td>30.00 ± 20.16</td>\n",
       "      <td>0.05 ± 0.22</td>\n",
       "      <td>3.97 ± 2.27</td>\n",
       "      <td>296.13 ± 113.57</td>\n",
       "      <td>3.82 ± 1.43</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta-llama/Meta-Llama-3.1-70B-Instruct</th>\n",
       "      <td>0.92 ± 0.22</td>\n",
       "      <td>35.00 ± 24.73</td>\n",
       "      <td>0.05 ± 0.22</td>\n",
       "      <td>4.85 ± 2.63</td>\n",
       "      <td>261.89 ± 109.02</td>\n",
       "      <td>9.42 ± 4.01</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfs-rag-baseline</th>\n",
       "      <td>0.90 ± 0.18</td>\n",
       "      <td>42.05 ± 26.91</td>\n",
       "      <td>0.21 ± 0.40</td>\n",
       "      <td>5.36 ± 2.59</td>\n",
       "      <td>119.41 ± 66.23</td>\n",
       "      <td>4.31 ± 1.80</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       answer_relevancy judge_completude  \\\n",
       "AgentPublic/llama3-instruct-8b              0.90 ± 0.23    26.41 ± 22.27   \n",
       "meta-llama/Meta-Llama-3.1-8B-Instruct       0.95 ± 0.15    30.00 ± 20.16   \n",
       "meta-llama/Meta-Llama-3.1-70B-Instruct      0.92 ± 0.22    35.00 ± 24.73   \n",
       "mfs-rag-baseline                            0.90 ± 0.18    42.05 ± 26.91   \n",
       "\n",
       "                                       judge_exactness judge_notator  \\\n",
       "AgentPublic/llama3-instruct-8b             0.03 ± 0.16   3.41 ± 1.97   \n",
       "meta-llama/Meta-Llama-3.1-8B-Instruct      0.05 ± 0.22   3.97 ± 2.27   \n",
       "meta-llama/Meta-Llama-3.1-70B-Instruct     0.05 ± 0.22   4.85 ± 2.63   \n",
       "mfs-rag-baseline                           0.21 ± 0.40   5.36 ± 2.59   \n",
       "\n",
       "                                          output_length generation_time  \\\n",
       "AgentPublic/llama3-instruct-8b           308.33 ± 93.23     6.64 ± 1.99   \n",
       "meta-llama/Meta-Llama-3.1-8B-Instruct   296.13 ± 113.57     3.82 ± 1.43   \n",
       "meta-llama/Meta-Llama-3.1-70B-Instruct  261.89 ± 109.02     9.42 ± 4.01   \n",
       "mfs-rag-baseline                         119.41 ± 66.23     4.31 ± 1.80   \n",
       "\n",
       "                                           toxicity         bias  \n",
       "AgentPublic/llama3-instruct-8b          0.00 ± 0.00  0.00 ± 0.00  \n",
       "meta-llama/Meta-Llama-3.1-8B-Instruct   0.00 ± 0.00  0.00 ± 0.00  \n",
       "meta-llama/Meta-Llama-3.1-70B-Instruct  0.00 ± 0.00  0.00 ± 0.00  \n",
       "mfs-rag-baseline                        0.00 ± 0.00  0.00 ± 0.00  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read results\n",
    "# --\n",
    "df_all = [] # list of results per experiment/model\n",
    "supports = [] # Store the support for debugging: if some answer/observation failed, the support can be less than the dataset size\n",
    "for model, exp_id in zip(models_to_test, experiment_ids):\n",
    "    # Get an experiment result\n",
    "    response = requests.get(f'{EVALAP_API_URL}/experiment/{exp_id}?with_results=true', headers=headers)\n",
    "    experiment = response.json()\n",
    "    # experiment[\"name\"] # Name of the experiment\n",
    "    if experiment[\"experiment_status\"] != \"finished\":\n",
    "        print(f\"Warning: experiment {exp_id} is not finished yet...\")\n",
    "    results = experiment[\"results\"]\n",
    "    \n",
    "    # Build a result dataframe  from the observation_table (mean, std etc)\n",
    "    df_metrics = {}\n",
    "    metric_support = []\n",
    "    supports.append(metric_support)\n",
    "    for metric_results in results: \n",
    "        metric_name = metric_results[\"metric_name\"]\n",
    "        arr = np.array([x[\"score\"] for x in metric_results[\"observation_table\"] if pd.notna(x[\"score\"])])\n",
    "        df = pd.DataFrame([[\n",
    "                np.mean(arr), # mean\n",
    "                np.std(arr), # std\n",
    "                np.median(arr), # median\n",
    "                f\"{arr.mean():.2f} ± {arr.std():.2f}\",  # Formatting as 'mean±std'\n",
    "                len(arr), # support\n",
    "            ]], columns=[\"mean\", \"std\", \"median\", \"mean_std\", \"support\"])\n",
    "        df_metrics[metric_name] = df\n",
    "        metric_support.append(len(arr))\n",
    "    \n",
    "    # Stack the mean_std final measure\n",
    "    name = model[\"_name\"] if model.get(\"_name\") else model[\"name\"]\n",
    "    df = pd.DataFrame({metric_name:df[\"mean_std\"].iloc[0] for metric_name, df in sorted(df_metrics.items())}, index=[name])\n",
    "    df_all.append(df)\n",
    "\n",
    "final_df = pd.concat(df_all)\n",
    "#final_df[\"support\"] = supports # for debugging\n",
    "# Reorder columns\n",
    "final_df = final_df[['answer_relevancy', 'judge_completude', 'judge_exactness', 'judge_notator', 'output_length', 'generation_time', 'toxicity', 'bias']]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688dcf6-2e78-48dd-85db-197bd4b767a6",
   "metadata": {},
   "source": [
    "## What is inside an experiment result ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6c3b601d-8340-49cc-aec5-fff803fb946c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1276,\n",
       " 'created_at': '2024-11-09T12:57:32.560445',\n",
       " 'score': 1.0,\n",
       " 'observation': 'The score is 1.00 because the response perfectly addresses the question without any irrelevant information. Great job!',\n",
       " 'num_line': 21,\n",
       " 'error_msg': None,\n",
       " 'execution_time': 4}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what's inside an experiment result\n",
    "# --\n",
    "metric_index = 2\n",
    "len(results) # number of metrics\n",
    "list(results[metric_index]) # the keys of the dict representing the result object\n",
    "len(results[metric_index][\"observation_table\"]) # number of observation -> one per dataset line.\n",
    "results[metric_index][\"observation_table\"][0] # the actual \"observation\" for one metric, for one line in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
