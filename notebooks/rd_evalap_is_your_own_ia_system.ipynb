{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33fde59-7aff-4f16-898d-95497d084548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from io import StringIO\n",
    "import concurrent.futures\n",
    "\n",
    "import dotenv\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from jinja2 import Template\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "\n",
    "EVALAP_API_URL = \"http://localhost:8000/v1\"\n",
    "#EVALAP_API_URL = \"https://evalap.etalab.gouv.fr/v1\"\n",
    "EVALAP_API_KEY = os.getenv(\"EVALAP_API_KEY\") \n",
    "ALBERT_API_URL = \"https://albert.api.etalab.gouv.fr/v1\"\n",
    "ALBERT_API_KEY = os.getenv(\"ALBERT_API_KEY\")\n",
    "OPENAI_URL = \"https://api.openai.com/v1\"\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "headers = {\"Authorization\": f\"Bearer {EVALAP_API_KEY}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb00e3-0309-4af1-b4e7-e92703de47c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the dataset from Evalap\n",
    "# --\n",
    "dataset_name = \"LegalBenchRAG\"\n",
    "response = requests.get(\n",
    "    f\"{EVALAP_API_URL}/dataset?name={dataset_name}&with_df=true\",\n",
    "    headers={\"Authorization\": f\"Bearer {EVALAP_API_KEY}\"},\n",
    ")\n",
    "response.raise_for_status()\n",
    "dataset = response.json()\n",
    "dataset_df =  pd.read_json(StringIO(dataset[\"df\"]))\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ad967-2543-4f4d-91d0-24abe091d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 1000 random index to work with a subset of the dataset in order to do faster and cheaper evaluation\n",
    "# --\n",
    "N = len(dataset_df) # Size of the \n",
    "rng = np.random.default_rng(42)\n",
    "sample = rng.choice(np.arange(N), size=1000, replace=False)\n",
    "sample = sample.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867c195-260b-4d93-9abc-2337c9ba7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial NORAG experiments\n",
    "# --\n",
    "\n",
    "expset_name = \"LegalBenchRAG Evaluation\"\n",
    "expset_readme = \"A extensive RAG evaluation on the LegalBecnhRAG dataset. See [complete me]\"\n",
    "\n",
    "system_prompt = \"Provide a clear and sound answer to the question. Use the source of information, if given, to answer.\"\n",
    "sampling_params = {\"temperature\": 0.2}\n",
    "\n",
    "common_params = {\n",
    "    \"dataset\": dataset[\"name\"],\n",
    "    \"metrics\": [\"judge_precision\", \"output_length\"],\n",
    "    \"model\": {\"sampling_params\" : sampling_params, \"system_prompt\": system_prompt},\n",
    "    \"judge_model\": {\n",
    "        \"name\": \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\", \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY\n",
    "    },\n",
    "    \"sample\": sample,\n",
    "}\n",
    "\n",
    "grid_params = {\n",
    "    \"model\": [\n",
    "        {\"name\": \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\", \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY}\n",
    "    ],\n",
    "}\n",
    "\n",
    "expset = {\n",
    "    \"name\": expset_name,\n",
    "    \"readme\": expset_readme,\n",
    "    \"cv\": {\"common_params\": common_params, \"grid_params\": grid_params, \"repeat\": 1},\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{EVALAP_API_URL}/experiment_set\",\n",
    "    headers={\"Authorization\": f\"Bearer {EVALAP_API_KEY}\", \"Content-Encoding\": \"gzip\"},\n",
    "    json=expset,\n",
    ")\n",
    "resp = response.json()\n",
    "if \"id\" in resp:\n",
    "    expset_id = resp[\"id\"]\n",
    "    print(f'Created expset: {resp[\"name\"]} ({resp[\"id\"]})')\n",
    "else:\n",
    "    print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d819c-d820-4584-ad9d-c9ec04a2d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the evalap repo into the workspace\n",
    "# @FUTURE: !pip install evalap\n",
    "sys.path.append(\"../\")\n",
    "from evalap.clients.llm import LlmClient, split_think_answer\n",
    "from evalap.rag.search import SearchEngineClient\n",
    "\n",
    "\n",
    "rag_prompt = Template(\"\"\"Use the following sources of information as a source of truth if they address the question:\n",
    "\n",
    "<SOURCE>\n",
    "{% for chunk in chunks %}\n",
    "{{chunk.text}}\n",
    "\n",
    "---\n",
    "\n",
    "{% endfor %}\n",
    "</SOURCE>\n",
    "\n",
    "QUESTION: \n",
    "{{query}}\n",
    "\"\"\")\n",
    "\n",
    "# Augment a prompt with collection search\n",
    "def do_rag(query, collection_name=None, limit=5, search_method=\"hybrid\", model_embedding=None):\n",
    "    # Search relevant chunks\n",
    "    se_client = SearchEngineClient()\n",
    "    hits = se_client.search(collection_name, query, limit=limit, method=search_method, model_embedding=model_embedding)\n",
    "    # Render prompt\n",
    "    return rag_prompt.render(query=query, chunks=hits, limit=limit)\n",
    "\n",
    "# The LLM core generation\n",
    "def generate_with_rag(prompt, model=None, system_prompt=system_prompt, with_rag=True, sampling_params=None, **rag_params):\n",
    "    if not sampling_params:\n",
    "        sampling_params = {}\n",
    "        \n",
    "    if with_rag:\n",
    "        prompt = do_rag(prompt, **rag_params)\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    if system_prompt:\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "        \n",
    "    aiclient = LlmClient()\n",
    "    result = aiclient.generate(model=model, messages=messages, **sampling_params)\n",
    "    observation = result.choices[0].message.content\n",
    "    think, answer = split_think_answer(observation)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b924c1-0f80-4f89-a266-07bafe389364",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(do_rag(\"CNI\", collection_name=\"legalbenchrag_v1\", model_embedding=\"BAAI/bge-m3\", limit=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44dfa6c-61c3-4bda-bd42-a17e9611b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async computing -- RAG generation\n",
    "# --\n",
    "from functools import partial\n",
    "\n",
    "# The models to runs\n",
    "models = [\n",
    "   {\n",
    "     \"aliased_name\": \"model_hybrid_7_bgem3\",\n",
    "     \"model\": \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\", \n",
    "     \"collection_name\": \"legalbenchrag_v1\",\n",
    "     \"model_embedding\": \"BAAI/bge-m3\",\n",
    "     \"search_method\": \"hybrid\",\n",
    "     \"limit\": 7,\n",
    "     \"system_prompt\": system_prompt,\n",
    "     \"sampling_params\": sampling_params,\n",
    "    },\n",
    "    {\n",
    "     \"aliased_name\": \"model_semantic_7_bgem3\",\n",
    "     \"model\": \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\", \n",
    "     \"collection_name\": \"legalbenchrag_v1\",\n",
    "     \"model_embedding\": \"BAAI/bge-m3\",\n",
    "     \"search_method\": \"semantic\",\n",
    "     \"limit\": 7,\n",
    "     \"system_prompt\": system_prompt,\n",
    "     \"sampling_params\": sampling_params,\n",
    "    },\n",
    "    {\n",
    "     \"aliased_name\": \"model_lexical_7_bgem3\",\n",
    "     \"model\": \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\", \n",
    "     \"collection_name\": \"legalbenchrag_v1\",\n",
    "     \"model_embedding\": \"BAAI/bge-m3\",\n",
    "     \"search_method\": \"lexical\",\n",
    "     \"limit\": 7,\n",
    "     \"system_prompt\": system_prompt,\n",
    "     \"sampling_params\": sampling_params,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Loop over the model to try\n",
    "model_answers = []\n",
    "for model in models:\n",
    "    # Create a list of model arguments (same model repeated for each prompt)\n",
    "    prompts = dataset_df.iloc[sample]['query'].tolist()\n",
    "    params_to_partial = {\n",
    "        k: model[k]\n",
    "        for k in [\"model\", \"collection_name\", \"model_embedding\", \"search_method\", \"limit\", \"system_prompt\", \"sampling_params\"] \n",
    "        if model.get(k)\n",
    "    }\n",
    "    generate_with_rag_partial = partial(generate_with_rag, **params_to_partial)\n",
    "\n",
    "    # Async over the prompts\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        # Map generate over pairs of (model, prompt)\n",
    "        results = list(executor.map(generate_with_rag_partial, prompts))\n",
    "\n",
    "    model_answers.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0af8e8-cc10-47da-9aaf-8a102156b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "for answers in model_answers:\n",
    "    print(len([x for x in answers if x.strip()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3744c5f-cae7-427a-a765-a630e6b0f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding RAG model to the experiment set\n",
    "# --\n",
    "common_params = {\n",
    "    \"dataset\": dataset[\"name\"],\n",
    "    \"metrics\": [\"judge_precision\", \"output_length\"],\n",
    "    \"model\": {\"sampling_params\" : sampling_params, \"system_prompt\": system_prompt},\n",
    "    \"judge_model\": {\n",
    "        \"name\": \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\", \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY\n",
    "    },\n",
    "    \"sample\": sample,\n",
    "}\n",
    "\n",
    "grid_params = {\n",
    "    \"model\": [\n",
    "        {\n",
    "            \"aliased_name\": model[\"aliased_name\"],\n",
    "            \"name\": model[\"model\"],\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"sampling_params\": sampling_params,\n",
    "            \"output\": model_answers[i],\n",
    "        }\n",
    "        for i, model in enumerate(models)    \n",
    "    ],\n",
    "}\n",
    "\n",
    "expset = {\n",
    "    \"cv\": {\"common_params\": common_params, \"grid_params\": grid_params, \"repeat\": 1},\n",
    "}\n",
    "\n",
    "response = requests.patch(\n",
    "    f\"{EVALAP_API_URL}/experiment_set/{expset_id}\",\n",
    "    headers={\"Authorization\": f\"Bearer {EVALAP_API_KEY}\", \"Content-Encoding\": \"gzip\"},\n",
    "    json=expset,\n",
    ")\n",
    "resp = response.json()\n",
    "if \"id\" in resp:\n",
    "    expset_id = resp[\"id\"]\n",
    "    print(f'Patch expset: {resp[\"name\"]} ({resp[\"id\"]})')\n",
    "else:\n",
    "    print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7768cf-97f5-4242-9319-863af47dd7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
