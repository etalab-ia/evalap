{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate you own IA system\n",
    "\n",
    "\n",
    "In this notebook, we will learn how to use **EvalAP**, the open source platform developed by Etalab, to evaluate a custom artificial intelligence (AI) system.\n",
    "\n",
    "You can use EvalAP for its evaluation component.\n",
    "\n",
    "How it works:\n",
    "- Your AI system generates its responses (and additional metadata if necessary) on your evaluation dataset.\n",
    "- You send these generations to EvalAP and create an experiment at the same time. EvalAP calculates the requested metrics.\n",
    "- You can see the results on the frontend.  \n",
    "\n",
    "Notes: You must have loaded the evaluation dataset once into EvalAP via the dataset endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > Your AI system generates responses in a dataframe 'res_for_evalap'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Use EvalAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "#config\n",
    "EVALAP_API_URL = \"http://localhost:8000\"\n",
    "EVALAP_API_KEY = os.getenv(\"EVALAP_API_KEY\") \n",
    "headers = {\"Authorization\": f\"Bearer {EVALAP_API_KEY}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Post dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step only needs to be done once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_dataset_to_api(name, readme, df, default_metric, columns_map=None, compliance=False):\n",
    "    dataset_payload = {\n",
    "        \"name\": name,\n",
    "        \"readme\": readme,\n",
    "        \"default_metric\": default_metric,\n",
    "        \"df\": df.to_json(orient=\"records\"),\n",
    "        \"compliance\": compliance\n",
    "    }\n",
    "    if columns_map:\n",
    "        dataset_payload[\"columns_map\"] = columns_map\n",
    "    try:\n",
    "        response = requests.post(f\"{EVALAP_API_URL}/v1/dataset\", json=dataset_payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        resp = response.json()\n",
    "        if \"id\" in resp:\n",
    "            print(f\"Dataset '{name}' publié avec succès (ID: {resp['id']})\")\n",
    "        else:\n",
    "            print(f\"Erreur de publication pour '{name}': {resp}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Erreur HTTP lors de la publication de '{name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is a df that contains :  \"question\", \"ground_truth\" and other columns \n",
    "\n",
    "post_dataset_to_api(\n",
    "    name=\"assistant-RH_head(2)\",\n",
    "    readme=\"'assistant RH -- Edouard Omar\",\n",
    "    df=dataset,\n",
    "    default_metric=\"judge_notator\",\n",
    "    columns_map={\"query\": \"question\", \"output_true\" : \"ground_truth\"},\n",
    "    compliance=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Run Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First RUN -- You have not completed an assessment on this AI system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_for_evalap contains \"answer\", \"generation_time\",\"contexts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE = \"gpt-4.1\"\n",
    "products = \"assistant-RH\"\n",
    "dataset_name = \"assistant-RH_head(2)\"\n",
    "\n",
    "expset_name = f\"{products}_run_{JUDGE}\"\n",
    "expset_readme = f\"evals for perfomring IA system of {products}\"\n",
    "\n",
    "metrics = [\"judge_notator\", \"faithfulness\", \"answer_relevancy\"]\n",
    "\n",
    "common_params = {\n",
    "    \"dataset\": dataset_name,\n",
    "    \"metrics\": metrics,\n",
    "    \"judge_model\": JUDGE,\n",
    "}\n",
    "grid_params = {\n",
    "    \"model\": [\n",
    "        {\n",
    "            \"aliased_name\": \"IA-system_RH\",\n",
    "            \"output\":res_for_evalap[\"answer\"].values.tolist(),\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "expset = {\n",
    "    \"name\": expset_name,\n",
    "    \"readme\": expset_readme,\n",
    "    \"cv\": {\"common_params\": common_params, \"grid_params\": grid_params, \"repeat\": 1},\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{EVALAP_API_URL}/v1/experiment_set\", json=expset, headers=headers)\n",
    "resp = response.json()\n",
    "if \"id\" in resp:\n",
    "    print(f'Created expset: {resp[\"name\"]} (ID: {resp[\"id\"]})')\n",
    "else:\n",
    "    print(f'Error creating experiment set for {dataset_name}: {resp}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch -- You have already completed at least one assessment on this AI system, and you want to complete new ones in the same place.  \n",
    "You must then enter the experiment number in the patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patching the experiment set\n",
    "\n",
    "output_list = res_for_evalap[\"answer\"].astype(str).tolist() \n",
    "time_list = res_for_evalap[\"generation_time\"].astype(int).values.tolist()\n",
    "nb_tokens_completion = res_for_evalap[\"generation_time\"].astype(int).values.tolist()\n",
    "#format context for EvalAP\n",
    "context_raw = res_for_evalap[\"contexts\"].astype(str).tolist()\n",
    "context_formatted = [[item] for item in context_raw]\n",
    "\n",
    "expset_id = 181\n",
    "\n",
    "metrics = [\"judge_precision\", \"judge_notator\", \"judge_exactness\",  \"answer_relevancy\",\n",
    "           \"faithfulness\", \"contextual_precision\", \"contextual_recall\", \"contextual_relevancy\"]\n",
    "           # \"ragas\"   ragas is no longer supported in deepeval\n",
    "\n",
    "common_params = {\n",
    "    \"dataset\" : dataset_name,\n",
    "    \"model\": {\"sampling_params\" : {\"temperature\": 0.2}},\n",
    "    \"metrics\" : metrics,\n",
    "    \"judge_model\": JUDGE,\n",
    "}\n",
    "\n",
    "grid_params = {\n",
    "    \"model\": [\n",
    "        {\n",
    "            \"aliased_name\": \"IA-system_RH+7+metrics\",\n",
    "            \"output\": json.loads(json.dumps(output_list)),\n",
    "            \"execution_time\": time_list, \n",
    "            \"nb_tokens_completion\": nb_tokens_completion,\n",
    "            #\"nb_tokens_prompt\": nb_tokens_prompt,\n",
    "            \"retrieval_context\": context_formatted #list of list \n",
    "            #\"context\": context_formatted #list of list \n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "expset = {\n",
    "    \"cv\": {\"common_params\": common_params, \"grid_params\": grid_params, \"repeat\":1}\n",
    "}\n",
    "response = requests.patch(f'{EVALAP_API_URL}/v1/experiment_set/{expset_id}', json=expset, headers=headers)\n",
    "resp = response.json()\n",
    "if \"id\" in resp:\n",
    "    expset_id = resp[\"id\"]\n",
    "    print(f'Patched expset: {resp[\"name\"]} ({resp[\"id\"]})')\n",
    "else:\n",
    "    print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see the result in the front : http://localhost:8501/experiments_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
